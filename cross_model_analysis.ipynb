{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epistemic Probing: Cross-Model Analysis\n",
    "\n",
    "This notebook analyzes epistemic transparency across 8 models (4 families Ã— base/instruct).\n",
    "\n",
    "**Key Question:** Language models know what they don't know. Does that knowledge leak through entropy, and how does it change after fine-tuning and across architectures?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "from analysis.loader import load_model_data\n",
    "from analysis.effects import compute_roc_auc\n",
    "from analysis.core import failure_mode_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model metadata\n",
    "MODELS = [\n",
    "    'qwen_base', 'qwen_instruct',\n",
    "    'mistral_base', 'mistral_instruct',\n",
    "    'yi_base', 'yi_instruct',\n",
    "    'llama_base', 'llama_instruct'\n",
    "]\n",
    "\n",
    "META = {\n",
    "    'qwen': ('Custom', 'Chinese'),\n",
    "    'mistral': ('Custom', 'English'),\n",
    "    'yi': ('LLaMA-derived', 'Chinese'),\n",
    "    'llama': ('LLaMA', 'English'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded qwen_base: 589 samples\n",
      "Loaded qwen_instruct: 589 samples\n",
      "Loaded mistral_base: 589 samples\n",
      "Loaded mistral_instruct: 589 samples\n",
      "Loaded yi_base: 589 samples\n",
      "Loaded yi_instruct: 589 samples\n",
      "Loaded llama_base: 589 samples\n",
      "Loaded llama_instruct: 589 samples\n"
     ]
    }
   ],
   "source": [
    "# Load all model data\n",
    "models_data = {}\n",
    "for model in MODELS:\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        models_data[model] = load_model_data(model, re_evaluate=True)\n",
    "    print(f\"Loaded {model}: {len(models_data[model].df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>family</th>\n",
       "      <th>variant</th>\n",
       "      <th>arch</th>\n",
       "      <th>training</th>\n",
       "      <th>entropy_auc</th>\n",
       "      <th>probe_auc</th>\n",
       "      <th>hidden_info</th>\n",
       "      <th>hall_det</th>\n",
       "      <th>mean_entropy</th>\n",
       "      <th>std_entropy</th>\n",
       "      <th>overall_acc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qwen_base</td>\n",
       "      <td>Qwen</td>\n",
       "      <td>base</td>\n",
       "      <td>Custom</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.763777</td>\n",
       "      <td>0.946349</td>\n",
       "      <td>0.182572</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>4.056761</td>\n",
       "      <td>1.119518</td>\n",
       "      <td>0.344652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qwen_instruct</td>\n",
       "      <td>Qwen</td>\n",
       "      <td>instruct</td>\n",
       "      <td>Custom</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.641253</td>\n",
       "      <td>0.945682</td>\n",
       "      <td>0.304428</td>\n",
       "      <td>0.585859</td>\n",
       "      <td>0.646161</td>\n",
       "      <td>0.458352</td>\n",
       "      <td>0.526316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral_base</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>base</td>\n",
       "      <td>Custom</td>\n",
       "      <td>English</td>\n",
       "      <td>0.923253</td>\n",
       "      <td>0.970427</td>\n",
       "      <td>0.047174</td>\n",
       "      <td>0.060606</td>\n",
       "      <td>2.803168</td>\n",
       "      <td>1.794821</td>\n",
       "      <td>0.395586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral_instruct</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>instruct</td>\n",
       "      <td>Custom</td>\n",
       "      <td>English</td>\n",
       "      <td>0.788595</td>\n",
       "      <td>0.944900</td>\n",
       "      <td>0.156305</td>\n",
       "      <td>0.282828</td>\n",
       "      <td>1.890673</td>\n",
       "      <td>0.846999</td>\n",
       "      <td>0.443124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yi_base</td>\n",
       "      <td>Yi</td>\n",
       "      <td>base</td>\n",
       "      <td>LLaMA-derived</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.845220</td>\n",
       "      <td>0.942850</td>\n",
       "      <td>0.097630</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>4.039220</td>\n",
       "      <td>1.385021</td>\n",
       "      <td>0.353141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yi_instruct</td>\n",
       "      <td>Yi</td>\n",
       "      <td>instruct</td>\n",
       "      <td>LLaMA-derived</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.695343</td>\n",
       "      <td>0.929878</td>\n",
       "      <td>0.234535</td>\n",
       "      <td>0.191919</td>\n",
       "      <td>1.153487</td>\n",
       "      <td>0.307822</td>\n",
       "      <td>0.409168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama_base</td>\n",
       "      <td>Llama</td>\n",
       "      <td>base</td>\n",
       "      <td>LLaMA</td>\n",
       "      <td>English</td>\n",
       "      <td>0.935200</td>\n",
       "      <td>0.958191</td>\n",
       "      <td>0.022990</td>\n",
       "      <td>0.070707</td>\n",
       "      <td>2.917209</td>\n",
       "      <td>1.850657</td>\n",
       "      <td>0.395586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama_instruct</td>\n",
       "      <td>Llama</td>\n",
       "      <td>instruct</td>\n",
       "      <td>LLaMA</td>\n",
       "      <td>English</td>\n",
       "      <td>0.738617</td>\n",
       "      <td>0.943124</td>\n",
       "      <td>0.204507</td>\n",
       "      <td>0.686869</td>\n",
       "      <td>2.143775</td>\n",
       "      <td>0.772969</td>\n",
       "      <td>0.534805</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model   family   variant           arch training  entropy_auc  \\\n",
       "0         qwen_base     Qwen      base         Custom  Chinese     0.763777   \n",
       "1     qwen_instruct     Qwen  instruct         Custom  Chinese     0.641253   \n",
       "2      mistral_base  Mistral      base         Custom  English     0.923253   \n",
       "3  mistral_instruct  Mistral  instruct         Custom  English     0.788595   \n",
       "4           yi_base       Yi      base  LLaMA-derived  Chinese     0.845220   \n",
       "5       yi_instruct       Yi  instruct  LLaMA-derived  Chinese     0.695343   \n",
       "6        llama_base    Llama      base          LLaMA  English     0.935200   \n",
       "7    llama_instruct    Llama  instruct          LLaMA  English     0.738617   \n",
       "\n",
       "   probe_auc  hidden_info  hall_det  mean_entropy  std_entropy  overall_acc  \n",
       "0   0.946349     0.182572  0.010101      4.056761     1.119518     0.344652  \n",
       "1   0.945682     0.304428  0.585859      0.646161     0.458352     0.526316  \n",
       "2   0.970427     0.047174  0.060606      2.803168     1.794821     0.395586  \n",
       "3   0.944900     0.156305  0.282828      1.890673     0.846999     0.443124  \n",
       "4   0.942850     0.097630  0.010101      4.039220     1.385021     0.353141  \n",
       "5   0.929878     0.234535  0.191919      1.153487     0.307822     0.409168  \n",
       "6   0.958191     0.022990  0.070707      2.917209     1.850657     0.395586  \n",
       "7   0.943124     0.204507  0.686869      2.143775     0.772969     0.534805  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute core metrics for all models\n",
    "results = []\n",
    "for model in MODELS:\n",
    "    data = models_data[model]\n",
    "    family = model.split('_')[0]\n",
    "    variant = model.split('_')[1]\n",
    "    \n",
    "    # ROC/AUC\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        roc = compute_roc_auc(data, print_output=False)\n",
    "    \n",
    "    # Hallucination detection\n",
    "    ci = data.df[data.df['category'] == 'confident_incorrect']\n",
    "    \n",
    "    results.append({\n",
    "        'model': model,\n",
    "        'family': family.capitalize(),\n",
    "        'variant': variant,\n",
    "        'arch': META[family][0],\n",
    "        'training': META[family][1],\n",
    "        'entropy_auc': roc['entropy']['auc'],\n",
    "        'probe_auc': roc['best_layer']['auc'],\n",
    "        'hidden_info': roc['best_layer']['auc'] - roc['entropy']['auc'],\n",
    "        'hall_det': ci['correct'].mean(),\n",
    "        'mean_entropy': data.df['entropy'].mean(),\n",
    "        'std_entropy': data.df['entropy'].std(),\n",
    "        'overall_acc': data.df['correct'].mean(),\n",
    "    })\n",
    "\n",
    "core_df = pd.DataFrame(results)\n",
    "core_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Comparison: Training Data vs Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>arch</th>\n",
       "      <th>training</th>\n",
       "      <th>entropy_auc</th>\n",
       "      <th>probe_auc</th>\n",
       "      <th>hidden_info</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Llama</td>\n",
       "      <td>LLaMA</td>\n",
       "      <td>English</td>\n",
       "      <td>0.935200</td>\n",
       "      <td>0.958191</td>\n",
       "      <td>0.022990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mistral</td>\n",
       "      <td>Custom</td>\n",
       "      <td>English</td>\n",
       "      <td>0.923253</td>\n",
       "      <td>0.970427</td>\n",
       "      <td>0.047174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Yi</td>\n",
       "      <td>LLaMA-derived</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.845220</td>\n",
       "      <td>0.942850</td>\n",
       "      <td>0.097630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Custom</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.763777</td>\n",
       "      <td>0.946349</td>\n",
       "      <td>0.182572</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    family           arch training  entropy_auc  probe_auc  hidden_info\n",
       "6    Llama          LLaMA  English     0.935200   0.958191     0.022990\n",
       "2  Mistral         Custom  English     0.923253   0.970427     0.047174\n",
       "4       Yi  LLaMA-derived  Chinese     0.845220   0.942850     0.097630\n",
       "0     Qwen         Custom  Chinese     0.763777   0.946349     0.182572"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Base models only - the clean comparison\n",
    "base_df = core_df[core_df['variant'] == 'base'][['family', 'arch', 'training', 'entropy_auc', 'probe_auc', 'hidden_info']]\n",
    "base_df = base_df.sort_values('hidden_info')\n",
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same LLaMA architecture, different training:\n",
      "family          arch training  entropy_auc  probe_auc  hidden_info\n",
      " Llama         LLaMA  English      0.93520   0.958191      0.02299\n",
      "    Yi LLaMA-derived  Chinese      0.84522   0.942850      0.09763\n",
      "\n",
      "Hidden info ratio: 4.2x\n"
     ]
    }
   ],
   "source": [
    "# The critical test: Yi vs Llama (same architecture, different training)\n",
    "yi_llama = base_df[base_df['family'].isin(['Yi', 'Llama'])]\n",
    "print(\"Same LLaMA architecture, different training:\")\n",
    "print(yi_llama.to_string(index=False))\n",
    "print(f\"\\nHidden info ratio: {yi_llama[yi_llama['family']=='Yi']['hidden_info'].values[0] / yi_llama[yi_llama['family']=='Llama']['hidden_info'].values[0]:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean hidden info by training origin (base models):\n",
      "training\n",
      "Chinese    0.140101\n",
      "English    0.035082\n",
      "Name: hidden_info, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Group by training origin\n",
    "print(\"Mean hidden info by training origin (base models):\")\n",
    "print(base_df.groupby('training')['hidden_info'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instruct Tuning Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>family</th>\n",
       "      <th>training</th>\n",
       "      <th>entropy_auc_delta</th>\n",
       "      <th>probe_auc_delta</th>\n",
       "      <th>hidden_info_delta</th>\n",
       "      <th>hall_det_delta</th>\n",
       "      <th>mean_entropy_delta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Qwen</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>-0.122523</td>\n",
       "      <td>-0.000667</td>\n",
       "      <td>0.121856</td>\n",
       "      <td>0.575758</td>\n",
       "      <td>-3.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mistral</td>\n",
       "      <td>English</td>\n",
       "      <td>-0.134659</td>\n",
       "      <td>-0.025527</td>\n",
       "      <td>0.109131</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>-0.912495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Yi</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>-0.149877</td>\n",
       "      <td>-0.012972</td>\n",
       "      <td>0.136905</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>-2.885733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama</td>\n",
       "      <td>English</td>\n",
       "      <td>-0.196584</td>\n",
       "      <td>-0.015067</td>\n",
       "      <td>0.181517</td>\n",
       "      <td>0.616162</td>\n",
       "      <td>-0.773433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    family training  entropy_auc_delta  probe_auc_delta  hidden_info_delta  \\\n",
       "0     Qwen  Chinese          -0.122523        -0.000667           0.121856   \n",
       "1  Mistral  English          -0.134659        -0.025527           0.109131   \n",
       "2       Yi  Chinese          -0.149877        -0.012972           0.136905   \n",
       "3    Llama  English          -0.196584        -0.015067           0.181517   \n",
       "\n",
       "   hall_det_delta  mean_entropy_delta  \n",
       "0        0.575758           -3.410600  \n",
       "1        0.222222           -0.912495  \n",
       "2        0.181818           -2.885733  \n",
       "3        0.616162           -0.773433  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute deltas for each family\n",
    "deltas = []\n",
    "for family in ['Qwen', 'Mistral', 'Yi', 'Llama']:\n",
    "    base = core_df[(core_df['family'] == family) & (core_df['variant'] == 'base')].iloc[0]\n",
    "    inst = core_df[(core_df['family'] == family) & (core_df['variant'] == 'instruct')].iloc[0]\n",
    "    \n",
    "    deltas.append({\n",
    "        'family': family,\n",
    "        'training': base['training'],\n",
    "        'entropy_auc_delta': inst['entropy_auc'] - base['entropy_auc'],\n",
    "        'probe_auc_delta': inst['probe_auc'] - base['probe_auc'],\n",
    "        'hidden_info_delta': inst['hidden_info'] - base['hidden_info'],\n",
    "        'hall_det_delta': inst['hall_det'] - base['hall_det'],\n",
    "        'mean_entropy_delta': inst['mean_entropy'] - base['mean_entropy'],\n",
    "    })\n",
    "\n",
    "delta_df = pd.DataFrame(deltas)\n",
    "delta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean effect of instruct tuning across all models:\n",
      "  Entropy AUC:  -0.151\n",
      "  Probe AUC:    -0.014\n",
      "  Hidden Info:  +13.7%\n",
      "  Hall. Det:    +39.9%\n",
      "  Mean Entropy: -1.996\n"
     ]
    }
   ],
   "source": [
    "# Summary of instruct tuning effects\n",
    "print(\"Mean effect of instruct tuning across all models:\")\n",
    "print(f\"  Entropy AUC:  {delta_df['entropy_auc_delta'].mean():+.3f}\")\n",
    "print(f\"  Probe AUC:    {delta_df['probe_auc_delta'].mean():+.3f}\")\n",
    "print(f\"  Hidden Info:  {delta_df['hidden_info_delta'].mean():+.1%}\")\n",
    "print(f\"  Hall. Det:    {delta_df['hall_det_delta'].mean():+.1%}\")\n",
    "print(f\"  Mean Entropy: {delta_df['mean_entropy_delta'].mean():+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entropy Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>variant</th>\n",
       "      <th>training</th>\n",
       "      <th>mean_entropy</th>\n",
       "      <th>std_entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>qwen_base</td>\n",
       "      <td>base</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>4.056761</td>\n",
       "      <td>1.119518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>qwen_instruct</td>\n",
       "      <td>instruct</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>0.646161</td>\n",
       "      <td>0.458352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mistral_base</td>\n",
       "      <td>base</td>\n",
       "      <td>English</td>\n",
       "      <td>2.803168</td>\n",
       "      <td>1.794821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mistral_instruct</td>\n",
       "      <td>instruct</td>\n",
       "      <td>English</td>\n",
       "      <td>1.890673</td>\n",
       "      <td>0.846999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yi_base</td>\n",
       "      <td>base</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>4.039220</td>\n",
       "      <td>1.385021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>yi_instruct</td>\n",
       "      <td>instruct</td>\n",
       "      <td>Chinese</td>\n",
       "      <td>1.153487</td>\n",
       "      <td>0.307822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>llama_base</td>\n",
       "      <td>base</td>\n",
       "      <td>English</td>\n",
       "      <td>2.917209</td>\n",
       "      <td>1.850657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>llama_instruct</td>\n",
       "      <td>instruct</td>\n",
       "      <td>English</td>\n",
       "      <td>2.143775</td>\n",
       "      <td>0.772969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model   variant training  mean_entropy  std_entropy\n",
       "0         qwen_base      base  Chinese      4.056761     1.119518\n",
       "1     qwen_instruct  instruct  Chinese      0.646161     0.458352\n",
       "2      mistral_base      base  English      2.803168     1.794821\n",
       "3  mistral_instruct  instruct  English      1.890673     0.846999\n",
       "4           yi_base      base  Chinese      4.039220     1.385021\n",
       "5       yi_instruct  instruct  Chinese      1.153487     0.307822\n",
       "6        llama_base      base  English      2.917209     1.850657\n",
       "7    llama_instruct  instruct  English      2.143775     0.772969"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy stats by model\n",
    "entropy_df = core_df[['model', 'variant', 'training', 'mean_entropy', 'std_entropy']].copy()\n",
    "entropy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>model</th>\n",
       "      <th>llama_base</th>\n",
       "      <th>llama_instruct</th>\n",
       "      <th>mistral_base</th>\n",
       "      <th>mistral_instruct</th>\n",
       "      <th>qwen_base</th>\n",
       "      <th>qwen_instruct</th>\n",
       "      <th>yi_base</th>\n",
       "      <th>yi_instruct</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ambiguous</th>\n",
       "      <td>4.61</td>\n",
       "      <td>2.60</td>\n",
       "      <td>4.14</td>\n",
       "      <td>2.63</td>\n",
       "      <td>4.90</td>\n",
       "      <td>1.04</td>\n",
       "      <td>5.26</td>\n",
       "      <td>1.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confident_correct</th>\n",
       "      <td>1.00</td>\n",
       "      <td>1.51</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.11</td>\n",
       "      <td>3.29</td>\n",
       "      <td>0.42</td>\n",
       "      <td>2.73</td>\n",
       "      <td>0.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>confident_incorrect</th>\n",
       "      <td>4.30</td>\n",
       "      <td>2.92</td>\n",
       "      <td>4.43</td>\n",
       "      <td>2.76</td>\n",
       "      <td>4.88</td>\n",
       "      <td>0.80</td>\n",
       "      <td>4.51</td>\n",
       "      <td>1.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nonsensical</th>\n",
       "      <td>4.50</td>\n",
       "      <td>2.57</td>\n",
       "      <td>4.48</td>\n",
       "      <td>2.27</td>\n",
       "      <td>5.08</td>\n",
       "      <td>0.58</td>\n",
       "      <td>4.95</td>\n",
       "      <td>1.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uncertain_correct</th>\n",
       "      <td>1.17</td>\n",
       "      <td>1.57</td>\n",
       "      <td>1.05</td>\n",
       "      <td>1.39</td>\n",
       "      <td>3.75</td>\n",
       "      <td>0.39</td>\n",
       "      <td>3.21</td>\n",
       "      <td>1.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>uncertain_incorrect</th>\n",
       "      <td>3.59</td>\n",
       "      <td>2.19</td>\n",
       "      <td>3.23</td>\n",
       "      <td>1.79</td>\n",
       "      <td>3.25</td>\n",
       "      <td>0.80</td>\n",
       "      <td>4.70</td>\n",
       "      <td>1.21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "model                llama_base  llama_instruct  mistral_base  \\\n",
       "category                                                        \n",
       "ambiguous                  4.61            2.60          4.14   \n",
       "confident_correct          1.00            1.51          1.05   \n",
       "confident_incorrect        4.30            2.92          4.43   \n",
       "nonsensical                4.50            2.57          4.48   \n",
       "uncertain_correct          1.17            1.57          1.05   \n",
       "uncertain_incorrect        3.59            2.19          3.23   \n",
       "\n",
       "model                mistral_instruct  qwen_base  qwen_instruct  yi_base  \\\n",
       "category                                                                   \n",
       "ambiguous                        2.63       4.90           1.04     5.26   \n",
       "confident_correct                1.11       3.29           0.42     2.73   \n",
       "confident_incorrect              2.76       4.88           0.80     4.51   \n",
       "nonsensical                      2.27       5.08           0.58     4.95   \n",
       "uncertain_correct                1.39       3.75           0.39     3.21   \n",
       "uncertain_incorrect              1.79       3.25           0.80     4.70   \n",
       "\n",
       "model                yi_instruct  \n",
       "category                          \n",
       "ambiguous                   1.24  \n",
       "confident_correct           0.96  \n",
       "confident_incorrect         1.28  \n",
       "nonsensical                 1.38  \n",
       "uncertain_correct           1.03  \n",
       "uncertain_incorrect         1.21  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Entropy by category for each model\n",
    "cat_entropy = []\n",
    "for model, data in models_data.items():\n",
    "    for cat in data.df['category'].unique():\n",
    "        cat_df = data.df[data.df['category'] == cat]\n",
    "        cat_entropy.append({\n",
    "            'model': model,\n",
    "            'category': cat,\n",
    "            'mean_entropy': cat_df['entropy'].mean(),\n",
    "            'accuracy': cat_df['correct'].mean(),\n",
    "            'n': len(cat_df)\n",
    "        })\n",
    "\n",
    "cat_entropy_df = pd.DataFrame(cat_entropy)\n",
    "cat_entropy_df.pivot(index='category', columns='model', values='mean_entropy').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hallucination Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m hall_df = core_df[[\u001b[33m'\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mvariant\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mtraining\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mhall_det\u001b[39m\u001b[33m'\u001b[39m]].copy()\n\u001b[32m      3\u001b[39m hall_df[\u001b[33m'\u001b[39m\u001b[33mhall_det_pct\u001b[39m\u001b[33m'\u001b[39m] = (hall_df[\u001b[33m'\u001b[39m\u001b[33mhall_det\u001b[39m\u001b[33m'\u001b[39m] * \u001b[32m100\u001b[39m).round(\u001b[32m1\u001b[39m).astype(\u001b[38;5;28mstr\u001b[39m) + \u001b[33m'\u001b[39m\u001b[33m%\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mhall_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtraining\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mvariant\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhall_det\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m.round(\u001b[32m3\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/epistemic_status/venv/lib/python3.13/site-packages/pandas/core/frame.py:9366\u001b[39m, in \u001b[36mDataFrame.pivot\u001b[39m\u001b[34m(self, columns, index, values)\u001b[39m\n\u001b[32m   9359\u001b[39m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   9360\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[33m\"\u001b[39m\u001b[33mpivot\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   9361\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpivot\u001b[39m(\n\u001b[32m   9362\u001b[39m     \u001b[38;5;28mself\u001b[39m, *, columns, index=lib.no_default, values=lib.no_default\n\u001b[32m   9363\u001b[39m ) -> DataFrame:\n\u001b[32m   9364\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpivot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pivot\n\u001b[32m-> \u001b[39m\u001b[32m9366\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpivot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/epistemic_status/venv/lib/python3.13/site-packages/pandas/core/reshape/pivot.py:570\u001b[39m, in \u001b[36mpivot\u001b[39m\u001b[34m(data, columns, index, values)\u001b[39m\n\u001b[32m    566\u001b[39m         indexed = data._constructor_sliced(data[values]._values, index=multiindex)\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# error: Argument 1 to \"unstack\" of \"DataFrame\" has incompatible type \"Union\u001b[39;00m\n\u001b[32m    568\u001b[39m \u001b[38;5;66;03m# [List[Any], ExtensionArray, ndarray[Any, Any], Index, Series]\"; expected\u001b[39;00m\n\u001b[32m    569\u001b[39m \u001b[38;5;66;03m# \"Hashable\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m570\u001b[39m result = \u001b[43mindexed\u001b[49m\u001b[43m.\u001b[49m\u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns_listlike\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    571\u001b[39m result.index.names = [\n\u001b[32m    572\u001b[39m     name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib.no_default \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m result.index.names\n\u001b[32m    573\u001b[39m ]\n\u001b[32m    575\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/epistemic_status/venv/lib/python3.13/site-packages/pandas/core/series.py:4634\u001b[39m, in \u001b[36mSeries.unstack\u001b[39m\u001b[34m(self, level, fill_value, sort)\u001b[39m\n\u001b[32m   4589\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4590\u001b[39m \u001b[33;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[32m   4591\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4630\u001b[39m \u001b[33;03mb    2    4\u001b[39;00m\n\u001b[32m   4631\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4632\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mreshape\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[32m-> \u001b[39m\u001b[32m4634\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43munstack\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/epistemic_status/venv/lib/python3.13/site-packages/pandas/core/reshape/reshape.py:517\u001b[39m, in \u001b[36munstack\u001b[39m\u001b[34m(obj, level, fill_value, sort)\u001b[39m\n\u001b[32m    515\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(obj.dtype):\n\u001b[32m    516\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value, sort=sort)\n\u001b[32m--> \u001b[39m\u001b[32m517\u001b[39m unstacker = \u001b[43m_Unstacker\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconstructor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_constructor_expanddim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\n\u001b[32m    519\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    520\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker.get_result(\n\u001b[32m    521\u001b[39m     obj._values, value_columns=\u001b[38;5;28;01mNone\u001b[39;00m, fill_value=fill_value\n\u001b[32m    522\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/epistemic_status/venv/lib/python3.13/site-packages/pandas/core/reshape/reshape.py:154\u001b[39m, in \u001b[36m_Unstacker.__init__\u001b[39m\u001b[34m(self, index, level, constructor, sort)\u001b[39m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_cells > np.iinfo(np.int32).max:\n\u001b[32m    147\u001b[39m     warnings.warn(\n\u001b[32m    148\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe following operation may generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m cells \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    149\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33min the resulting pandas object.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    150\u001b[39m         PerformanceWarning,\n\u001b[32m    151\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    152\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_selectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/epistemic_status/venv/lib/python3.13/site-packages/pandas/core/reshape/reshape.py:210\u001b[39m, in \u001b[36m_Unstacker._make_selectors\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    207\u001b[39m mask.put(selector, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.sum() < \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.index):\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIndex contains duplicate entries, cannot reshape\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    212\u001b[39m \u001b[38;5;28mself\u001b[39m.group_index = comp_index\n\u001b[32m    213\u001b[39m \u001b[38;5;28mself\u001b[39m.mask = mask\n",
      "\u001b[31mValueError\u001b[39m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "# Hallucination detection rates\n",
    "hall_df = core_df[['model', 'variant', 'training', 'hall_det']].copy()\n",
    "hall_df['hall_det_pct'] = (hall_df['hall_det'] * 100).round(1).astype(str) + '%'\n",
    "hall_df.pivot(index='training', columns='variant', values='hall_det').round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample hallucination responses from best and worst models\n",
    "best_model = core_df.loc[core_df['hall_det'].idxmax(), 'model']\n",
    "worst_model = core_df.loc[core_df[core_df['variant']=='instruct']['hall_det'].idxmin(), 'model']\n",
    "\n",
    "print(f\"Best hallucination detection: {best_model}\")\n",
    "print(f\"Worst instruct hallucination detection: {worst_model}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table for paper/presentation\n",
    "summary = core_df[['model', 'training', 'entropy_auc', 'probe_auc', 'hidden_info', 'hall_det', 'mean_entropy']].copy()\n",
    "summary.columns = ['Model', 'Training', 'Entropy AUC', 'Probe AUC', 'Hidden Info', 'Hall. Det', 'Mean Entropy']\n",
    "summary = summary.round(3)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV if needed\n",
    "# summary.to_csv('epistemic_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "1. **Training data drives epistemic transparency, not architecture**\n",
    "   - Yi (LLaMA arch, Chinese): ~10% hidden info\n",
    "   - Llama (LLaMA arch, English): ~2% hidden info\n",
    "   - Same architecture, 4x difference\n",
    "\n",
    "2. **Instruct tuning degrades entropy informativeness universally**\n",
    "   - All models show +10-18% hidden info after instruct tuning\n",
    "   - Entropy becomes compressed (lower mean and SD)\n",
    "\n",
    "3. **Probe accuracy remains stable**\n",
    "   - ~94-97% AUC across all models\n",
    "   - Information exists internally, just hidden from entropy\n",
    "\n",
    "4. **Hallucination detection improves with instruct tuning**\n",
    "   - But varies widely by model (19-69%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
