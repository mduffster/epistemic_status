{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epistemic Probing: Cross-Model Analysis\n",
    "\n",
    "This notebook analyzes epistemic transparency across 8 models (4 families Ã— base/instruct).\n",
    "\n",
    "**Key Question:** Do language models know what they don't know, and does that knowledge leak through entropy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import io\n",
    "from contextlib import redirect_stdout\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "sys.path.insert(0, '.')\n",
    "from analysis.loader import load_model_data\n",
    "from analysis.effects import compute_roc_auc\n",
    "from analysis.core import failure_mode_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model metadata\n",
    "MODELS = [\n",
    "    'qwen_base', 'qwen_instruct',\n",
    "    'mistral_base', 'mistral_instruct',\n",
    "    'yi_base', 'yi_instruct',\n",
    "    'llama_base', 'llama_instruct'\n",
    "]\n",
    "\n",
    "META = {\n",
    "    'qwen': ('Custom', 'Chinese'),\n",
    "    'mistral': ('Custom', 'English'),\n",
    "    'yi': ('LLaMA-derived', 'Chinese'),\n",
    "    'llama': ('LLaMA', 'English'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all model data\n",
    "models_data = {}\n",
    "for model in MODELS:\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        models_data[model] = load_model_data(model, re_evaluate=True)\n",
    "    print(f\"Loaded {model}: {len(models_data[model].df)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Core Metrics Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute core metrics for all models\n",
    "results = []\n",
    "for model in MODELS:\n",
    "    data = models_data[model]\n",
    "    family = model.split('_')[0]\n",
    "    variant = model.split('_')[1]\n",
    "    \n",
    "    # ROC/AUC\n",
    "    f = io.StringIO()\n",
    "    with redirect_stdout(f):\n",
    "        roc = compute_roc_auc(data, print_output=False)\n",
    "    \n",
    "    # Hallucination detection\n",
    "    ci = data.df[data.df['category'] == 'confident_incorrect']\n",
    "    \n",
    "    results.append({\n",
    "        'model': model,\n",
    "        'family': family.capitalize(),\n",
    "        'variant': variant,\n",
    "        'arch': META[family][0],\n",
    "        'training': META[family][1],\n",
    "        'entropy_auc': roc['entropy']['auc'],\n",
    "        'probe_auc': roc['best_layer']['auc'],\n",
    "        'hidden_info': roc['best_layer']['auc'] - roc['entropy']['auc'],\n",
    "        'hall_det': ci['correct'].mean(),\n",
    "        'mean_entropy': data.df['entropy'].mean(),\n",
    "        'std_entropy': data.df['entropy'].std(),\n",
    "        'overall_acc': data.df['correct'].mean(),\n",
    "    })\n",
    "\n",
    "core_df = pd.DataFrame(results)\n",
    "core_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Key Comparison: Training Data vs Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base models only - the clean comparison\n",
    "base_df = core_df[core_df['variant'] == 'base'][['family', 'arch', 'training', 'entropy_auc', 'probe_auc', 'hidden_info']]\n",
    "base_df = base_df.sort_values('hidden_info')\n",
    "base_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The critical test: Yi vs Llama (same architecture, different training)\n",
    "yi_llama = base_df[base_df['family'].isin(['Yi', 'Llama'])]\n",
    "print(\"Same LLaMA architecture, different training:\")\n",
    "print(yi_llama.to_string(index=False))\n",
    "print(f\"\\nHidden info ratio: {yi_llama[yi_llama['family']=='Yi']['hidden_info'].values[0] / yi_llama[yi_llama['family']=='Llama']['hidden_info'].values[0]:.1f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by training origin\n",
    "print(\"Mean hidden info by training origin (base models):\")\n",
    "print(base_df.groupby('training')['hidden_info'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instruct Tuning Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute deltas for each family\n",
    "deltas = []\n",
    "for family in ['Qwen', 'Mistral', 'Yi', 'Llama']:\n",
    "    base = core_df[(core_df['family'] == family) & (core_df['variant'] == 'base')].iloc[0]\n",
    "    inst = core_df[(core_df['family'] == family) & (core_df['variant'] == 'instruct')].iloc[0]\n",
    "    \n",
    "    deltas.append({\n",
    "        'family': family,\n",
    "        'training': base['training'],\n",
    "        'entropy_auc_delta': inst['entropy_auc'] - base['entropy_auc'],\n",
    "        'probe_auc_delta': inst['probe_auc'] - base['probe_auc'],\n",
    "        'hidden_info_delta': inst['hidden_info'] - base['hidden_info'],\n",
    "        'hall_det_delta': inst['hall_det'] - base['hall_det'],\n",
    "        'mean_entropy_delta': inst['mean_entropy'] - base['mean_entropy'],\n",
    "    })\n",
    "\n",
    "delta_df = pd.DataFrame(deltas)\n",
    "delta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of instruct tuning effects\n",
    "print(\"Mean effect of instruct tuning across all models:\")\n",
    "print(f\"  Entropy AUC:  {delta_df['entropy_auc_delta'].mean():+.3f}\")\n",
    "print(f\"  Probe AUC:    {delta_df['probe_auc_delta'].mean():+.3f}\")\n",
    "print(f\"  Hidden Info:  {delta_df['hidden_info_delta'].mean():+.1%}\")\n",
    "print(f\"  Hall. Det:    {delta_df['hall_det_delta'].mean():+.1%}\")\n",
    "print(f\"  Mean Entropy: {delta_df['mean_entropy_delta'].mean():+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Entropy Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy stats by model\n",
    "entropy_df = core_df[['model', 'variant', 'training', 'mean_entropy', 'std_entropy']].copy()\n",
    "entropy_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entropy by category for each model\n",
    "cat_entropy = []\n",
    "for model, data in models_data.items():\n",
    "    for cat in data.df['category'].unique():\n",
    "        cat_df = data.df[data.df['category'] == cat]\n",
    "        cat_entropy.append({\n",
    "            'model': model,\n",
    "            'category': cat,\n",
    "            'mean_entropy': cat_df['entropy'].mean(),\n",
    "            'accuracy': cat_df['correct'].mean(),\n",
    "            'n': len(cat_df)\n",
    "        })\n",
    "\n",
    "cat_entropy_df = pd.DataFrame(cat_entropy)\n",
    "cat_entropy_df.pivot(index='category', columns='model', values='mean_entropy').round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hallucination Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Hallucination detection rates by model\nhall_df = core_df[['model', 'family', 'variant', 'training', 'hall_det']].copy()\nhall_df['hall_det_pct'] = (hall_df['hall_det'] * 100).round(1).astype(str) + '%'\n\n# Pivot by family (unique) instead of training (duplicates)\nhall_pivot = hall_df.pivot(index='family', columns='variant', values='hall_det').round(3)\nhall_pivot['improvement'] = hall_pivot['instruct'] - hall_pivot['base']\nhall_pivot.sort_values('instruct', ascending=False)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Best and worst hallucination detection\nbest_model = core_df.loc[core_df['hall_det'].idxmax(), 'model']\nbest_rate = core_df['hall_det'].max()\n\ninstruct_only = core_df[core_df['variant']=='instruct']\nworst_instruct = instruct_only.loc[instruct_only['hall_det'].idxmin(), 'model']\nworst_rate = instruct_only['hall_det'].min()\n\nprint(f\"Best hallucination detection: {best_model} ({best_rate:.1%})\")\nprint(f\"Worst instruct model: {worst_instruct} ({worst_rate:.1%})\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary table for paper/presentation\n",
    "summary = core_df[['model', 'training', 'entropy_auc', 'probe_auc', 'hidden_info', 'hall_det', 'mean_entropy']].copy()\n",
    "summary.columns = ['Model', 'Training', 'Entropy AUC', 'Probe AUC', 'Hidden Info', 'Hall. Det', 'Mean Entropy']\n",
    "summary = summary.round(3)\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to CSV if needed\n",
    "# summary.to_csv('epistemic_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings\n",
    "\n",
    "1. **Training data drives epistemic transparency, not architecture**\n",
    "   - Yi (LLaMA arch, Chinese): ~10% hidden info\n",
    "   - Llama (LLaMA arch, English): ~2% hidden info\n",
    "   - Same architecture, 4x difference\n",
    "\n",
    "2. **Instruct tuning degrades entropy informativeness universally**\n",
    "   - All models show +10-18% hidden info after instruct tuning\n",
    "   - Entropy becomes compressed (lower mean and SD)\n",
    "\n",
    "3. **Probe accuracy remains stable**\n",
    "   - ~94-97% AUC across all models\n",
    "   - Information exists internally, just hidden from entropy\n",
    "\n",
    "4. **Hallucination detection improves with instruct tuning**\n",
    "   - But varies widely by model (19-69%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}